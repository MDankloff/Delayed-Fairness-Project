{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a91878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../src')\n",
    "\n",
    "from generator import *\n",
    "from evaluation import *\n",
    "from fair_model import FairModel\n",
    "from baselines import LR, CvxFairModel, EOFairModel\n",
    "from utils import gen_plot_data, plot_axes, combine_tuples,combine_tuples_active_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4e1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2935e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bank model\n",
    "bank = Bank()\n",
    "agent_train = Agent(n_samples=4000, protect_ratio=0.5, eps=0.5, base=[0.2, 1.0], seed=2026)\n",
    "agent_test = Agent(n_samples=1000, protect_ratio=0.5, eps=0.5, base=[0.2, 1.0], seed=2027)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bce72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 5\n",
    "\n",
    "s_train, adj, edges, Xs_train, Ys_train, Ds_train, Ps, Os, Us, As_train = run_simulation(\n",
    "    decision_model=bank,\n",
    "    repayment_model=bank,\n",
    "    agent=agent_train,\n",
    "    steps=steps,\n",
    "    enforce_demographic_mixing=True,\n",
    "    k_same=8,\n",
    "    k_other=2,\n",
    "    directed=False,\n",
    "    graph_seed=2026,\n",
    "    seed=2026,\n",
    "    decision_coef=0.8,\n",
    "    repayment_coef=0.8,\n",
    " )\n",
    "\n",
    "s_test, adj, edges, Xs_test, Ys_test, Ds_test, Ps, Os, Us, As_test = run_simulation(\n",
    "    decision_model=bank,\n",
    "    repayment_model=bank,\n",
    "    agent=agent_test,\n",
    "    steps=steps,\n",
    "    enforce_demographic_mixing=True,\n",
    "    k_same=8,\n",
    "    k_other=2,\n",
    "    directed=False,\n",
    "    graph_seed=2026,\n",
    "    seed=2026,\n",
    "    decision_coef=0.8,\n",
    "    repayment_coef=0.8,\n",
    " )\n",
    "s_comb, X_comb, Y_comb = combine_tuples_active_only(s_train, Xs_train, Ys_train,As_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab18ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "# out_path = \"../src/simulation_results_neighbors.csv\"\n",
    "# save_agent_panel_csv(\n",
    "#     out_path,\n",
    "#     s=s,\n",
    "#     Xs=Xs,\n",
    "#     adj=adj,\n",
    "#     Ds=Ds,\n",
    "#     Ys=Ys,\n",
    "#     Ps=Ps,\n",
    "#     Us=Us,\n",
    "#     As=As,\n",
    "#     Os=Os,\n",
    "#     t0=0,\n",
    "#     neighbor_k=10,\n",
    "#  )\n",
    "\n",
    "# print(\"Wrote:\", out_path)\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    # Build the combined data frame as before\n",
    "    data = np.hstack([s_comb.reshape(-1, 1), X_comb, Y_comb.reshape(-1, 1)])\n",
    "    base_columns = [\"s\"] + [f\"x{i}\" for i in range(X_comb.shape[1])] + [\"y\"]\n",
    "    df = pd.DataFrame(data, columns=base_columns)\n",
    "\n",
    "    # Infer per-step sizes from the original lists (Xs_train/Ys_train)\n",
    "    # Each Xs_train[t] is (n_t, d); we accumulate to compute global row indices per step.\n",
    "    step_sizes = [len(x_step) for x_step in Xs_train]\n",
    "    cum = np.cumsum([0] + step_sizes)  # boundaries\n",
    "\n",
    "    # Create arrays for (step, i-in-step)\n",
    "    idx = np.arange(len(df))\n",
    "    # Find step for each global index using cumulative boundaries\n",
    "    step = np.searchsorted(cum[1:], idx, side=\"right\")\n",
    "    i_in_step = idx - cum[step]\n",
    "\n",
    "    df.insert(0, \"step\", step)\n",
    "    df.insert(1, \"i\", i_in_step)\n",
    "    df.insert(2, \"comb_idx\", idx)\n",
    "\n",
    "    # Show first 10 rows with mapping columns\n",
    "    print(df.head(10).to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(\"Failed to annotate with step/index due to:\", e)\n",
    "    # Fallback to the simple 10-row preview\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        data = np.hstack([s_comb.reshape(-1, 1), X_comb, Y_comb.reshape(-1, 1)])\n",
    "        columns = [\"s\"] + [f\"x{i}\" for i in range(X_comb.shape[1])] + [\"y\"]\n",
    "        df_preview = pd.DataFrame(data, columns=columns)\n",
    "        print(df_preview.head(10).to_string(index=False))\n",
    "    except Exception as e2:\n",
    "        print(\"pandas not available (\", e2, \") â€” showing raw arrays instead:\\n\")\n",
    "        print(\"s:\\n\", s_comb[:10])\n",
    "        print(\"\\nX:\\n\", X_comb[:10])\n",
    "        print(\"\\ny:\\n\", Y_comb[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d4b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print s, x0, x1, y for steps 0..5 at i=1 and i=2\n",
    "import numpy as np\n",
    "\n",
    "# Derive per-step sizes and cumulative boundaries from Xs_train\n",
    "step_sizes = [len(x_step) for x_step in Xs_train]\n",
    "cum = np.cumsum([0] + step_sizes)\n",
    "\n",
    "steps_to_show = [0, 1, 2, 3, 4, 5]\n",
    "indices_to_show = [1, 2]\n",
    "\n",
    "rows = []\n",
    "for t in steps_to_show:\n",
    "    if t < 0 or t >= len(step_sizes):\n",
    "        rows.append((t, None, None, None, None, None, \"step out of range\"))\n",
    "        continue\n",
    "    for i in indices_to_show:\n",
    "        if i < 0 or i >= step_sizes[t]:\n",
    "            rows.append((t, i, None, None, None, None, \"i out of range for this step\"))\n",
    "            continue\n",
    "        gi = cum[t] + i  # global index in combined arrays\n",
    "        s_val = float(s_comb[gi])\n",
    "        x0 = float(X_comb[gi, 0])\n",
    "        x1 = float(X_comb[gi, 1])\n",
    "        y_val = float(Y_comb[gi])\n",
    "        rows.append((t, i, s_val, x0, x1, y_val, \"\"))\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    out_df = pd.DataFrame(rows, columns=[\"step\", \"i\", \"s\", \"x0\", \"x1\", \"y\", \"note\"])\n",
    "    print(out_df.to_string(index=False))\n",
    "except Exception:\n",
    "    for r in rows:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29224b08",
   "metadata": {},
   "source": [
    "### Baseline: LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949e7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Step 1 - Logistic Regression ------------------------------\n",
      "Acc: 70.7%\n",
      "Retention: 100.0%\n",
      "Short Fairness: 0.106\n",
      "Long fairness: 0.060\n",
      "------------------------------ Step 2 - Logistic Regression ------------------------------\n",
      "Acc: 73.0%\n",
      "Retention: 71.6%\n",
      "Short Fairness: 0.108\n",
      "Long fairness: 0.125\n",
      "------------------------------ Step 3 - Logistic Regression ------------------------------\n",
      "Acc: 73.3%\n",
      "Retention: 43.6%\n",
      "Short Fairness: 0.108\n",
      "Long fairness: 0.200\n",
      "------------------------------ Step 4 - Logistic Regression ------------------------------\n",
      "Acc: 70.4%\n",
      "Retention: 33.5%\n",
      "Short Fairness: 0.110\n",
      "Long fairness: 0.379\n",
      "------------------------------ Step 5 - Logistic Regression ------------------------------\n",
      "Acc: 72.9%\n",
      "Retention: 26.5%\n",
      "Short Fairness: 0.110\n",
      "Long fairness: 0.699\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LR(l2_reg=1e-5)\n",
    "lr.train(s_comb, X_comb, Y_comb)\n",
    "\n",
    "steps = 5\n",
    "\n",
    "# run_simulation returns: s, adj, edges, Xs, Ys, Ds, Ps, Os, Us, As\n",
    "s, adj, edges, Xs, Ys, Ds, Ps, Os, Us, As = run_simulation(\n",
    "    decision_model=lr,\n",
    "    repayment_model=bank,\n",
    "    agent=agent_test,\n",
    "    steps=steps,\n",
    "    enforce_demographic_mixing=True,\n",
    "    k_same=8,\n",
    "    k_other=2,\n",
    "    directed=False,\n",
    "    graph_seed=2026,\n",
    "    seed=2026,\n",
    "    decision_coef=0.8,\n",
    "    repayment_coef=0.8,\n",
    " )\n",
    "\n",
    "compute_statistics(s, Xs, Ds, lr, OYs=Ys,As=As)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c55e0f",
   "metadata": {},
   "source": [
    "### Baseline: FMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ad1725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Step 1 - Fair Model with Demographic Parity ------------------------------\n",
      "Acc: 34.8%\n",
      "Retention: 100.0%\n",
      "Short Fairness: 0.006\n",
      "Long fairness: 0.000\n",
      "------------------------------ Step 2 - Fair Model with Demographic Parity ------------------------------\n",
      "Acc: 35.4%\n",
      "Retention: 82.2%\n",
      "Short Fairness: 0.006\n",
      "Long fairness: 0.001\n",
      "------------------------------ Step 3 - Fair Model with Demographic Parity ------------------------------\n",
      "Acc: 35.6%\n",
      "Retention: 58.4%\n",
      "Short Fairness: 0.006\n",
      "Long fairness: 0.199\n",
      "------------------------------ Step 4 - Fair Model with Demographic Parity ------------------------------\n",
      "Acc: 37.9%\n",
      "Retention: 45.1%\n",
      "Short Fairness: 0.006\n",
      "Long fairness: 0.974\n",
      "------------------------------ Step 5 - Fair Model with Demographic Parity ------------------------------\n",
      "Acc: 38.9%\n",
      "Retention: 35.5%\n",
      "Short Fairness: 0.006\n",
      "Long fairness: 3.874\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = X_comb.shape[1]            # number of X columns\n",
    "cfm = CvxFairModel(n_features=d + 1, l2_reg=1e-5, tao=1.565)  # s + X\n",
    "\n",
    "# cfm = CvxFairModel(n_features=len(Xs_train[0][0])+2, l2_reg=1e-5, tao=1.565)\n",
    "\n",
    "cfm.train(s_comb, X_comb, Y_comb)\n",
    "\n",
    "s, adj, edges, Xs, Ys, Ds, Ps, Os, Us, As = run_simulation(\n",
    "    decision_model=cfm,\n",
    "    repayment_model=bank,\n",
    "    agent=agent_test,\n",
    "    steps=steps,\n",
    "    enforce_demographic_mixing=True,\n",
    "    k_same=8,\n",
    "    k_other=2,\n",
    "    directed=False,\n",
    "    graph_seed=2026,\n",
    "    seed=2026,\n",
    "    decision_coef=0.8,\n",
    "    repayment_coef=0.8,\n",
    " )\n",
    "\n",
    "compute_statistics(s_test, Xs, Ds, cfm, OYs=Ys,As=As) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a5db6",
   "metadata": {},
   "source": [
    "## Baseline: FMEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0d59b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal\n",
      "------------------------------ Step 1 - Fair Model with Equal Oppertunity ------------------------------\n",
      "Acc: 70.1%\n",
      "Retention: 100.0%\n",
      "Short Fairness: 0.052\n",
      "Long fairness: 0.102\n",
      "------------------------------ Step 2 - Fair Model with Equal Oppertunity ------------------------------\n",
      "Acc: 72.3%\n",
      "Retention: 76.3%\n",
      "Short Fairness: 0.042\n",
      "Long fairness: 0.154\n",
      "------------------------------ Step 3 - Fair Model with Equal Oppertunity ------------------------------\n",
      "Acc: 72.0%\n",
      "Retention: 50.7%\n",
      "Short Fairness: 0.044\n",
      "Long fairness: 0.218\n",
      "------------------------------ Step 4 - Fair Model with Equal Oppertunity ------------------------------\n",
      "Acc: 70.6%\n",
      "Retention: 37.9%\n",
      "Short Fairness: 0.046\n",
      "Long fairness: 0.417\n",
      "------------------------------ Step 5 - Fair Model with Equal Oppertunity ------------------------------\n",
      "Acc: 71.6%\n",
      "Retention: 29.7%\n",
      "Short Fairness: 0.042\n",
      "Long fairness: 0.854\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eqm = EOFairModel(n_features=len(Xs_train[0][0])+2, l2_reg=1e-5, tao=1.5)\n",
    "eqm.train(s_comb, X_comb, Y_comb)\n",
    "\n",
    "s, adj, edges, Xs, Ys, Ds, Ps, Os, Us, As = run_simulation(\n",
    "    decision_model=eqm,\n",
    "    repayment_model=bank,\n",
    "    agent=agent_test,\n",
    "    steps=steps,\n",
    "    enforce_demographic_mixing=True,\n",
    "    k_same=8,\n",
    "    k_other=2,\n",
    "    directed=False,\n",
    "    graph_seed=2026,\n",
    "    seed=2026,\n",
    "    decision_coef=0.8,\n",
    "    repayment_coef=0.8,\n",
    " )\n",
    "\n",
    "compute_statistics(s_test, Xs, Ds, eqm, OYs=Ys,As=As) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047c1f3",
   "metadata": {},
   "source": [
    "### FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b344cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Done!\n"
     ]
    }
   ],
   "source": [
    "fm = FairModel(n_features=len(Xs_train[0][0])+1, lr=5e-3, l2_reg=1e-5, sf_reg=0.119, lf_reg=0.154)\n",
    "fm.train(s_train, Xs_train, Ys_train, Xs_train, Ys_train, epochs=1000, plot=False)\n",
    "\n",
    "num_iters = 50\n",
    "\n",
    "theta_true = fm.params\n",
    "theta_list     = [np.copy(theta_true)]\n",
    "theta_gaps     = []\n",
    "\n",
    "\n",
    "# inital theta\n",
    "theta = np.copy(theta_true)\n",
    "\n",
    "for t in range(num_iters):\n",
    "    # adjust distribution to current theta\n",
    "    s, adj, edges, Xs, Ys, Ds, Ps, Os, Us, As = run_simulation(\n",
    "    decision_model=fm,\n",
    "    repayment_model=bank,\n",
    "    agent=agent_train,\n",
    "    steps=steps,\n",
    "    enforce_demographic_mixing=True,\n",
    "    k_same=8,\n",
    "    k_other=2,\n",
    "    directed=False,\n",
    "    graph_seed=2026,\n",
    "    seed=2026,\n",
    "    decision_coef=0.8,\n",
    "    repayment_coef=0.8,\n",
    " )\n",
    "    # learn on induced distribution\n",
    "    fm.train(s_train, Xs_train, Ys_train, Xs, Ys, epochs=10, plot=False)\n",
    "    \n",
    "    # keep track of statistic\n",
    "    theta_new = fm.params\n",
    "    theta_gaps.append(np.linalg.norm(theta_new - theta))\n",
    "    theta_list.append(np.copy(theta_new))\n",
    "\n",
    "    theta = np.copy(theta_new)\n",
    "print(\"Retraining Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ede5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Step 1 - Long-term Fair Model ------------------------------\n",
      "Acc: 68.9%\n",
      "Retention: 100.0%\n",
      "Short Fairness: 0.132\n",
      "Long fairness: 0.074\n",
      "------------------------------ Step 2 - Long-term Fair Model ------------------------------\n",
      "Acc: 69.7%\n",
      "Retention: 79.7%\n",
      "Short Fairness: 0.130\n",
      "Long fairness: 0.102\n",
      "------------------------------ Step 3 - Long-term Fair Model ------------------------------\n",
      "Acc: 68.9%\n",
      "Retention: 55.9%\n",
      "Short Fairness: 0.130\n",
      "Long fairness: 0.110\n",
      "------------------------------ Step 4 - Long-term Fair Model ------------------------------\n",
      "Acc: 67.3%\n",
      "Retention: 43.1%\n",
      "Short Fairness: 0.136\n",
      "Long fairness: 0.218\n",
      "------------------------------ Step 5 - Long-term Fair Model ------------------------------\n",
      "Acc: 68.0%\n",
      "Retention: 32.7%\n",
      "Short Fairness: 0.134\n",
      "Long fairness: 0.443\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s, adj, edges, Xs, Ys, Ds, Ps, Os, Us, As = run_simulation(\n",
    "    decision_model=fm,\n",
    "    repayment_model=bank,\n",
    "    agent=agent_test,\n",
    "    steps=steps,\n",
    "    enforce_demographic_mixing=True,\n",
    "    k_same=8,\n",
    "    k_other=2,\n",
    "    directed=False,\n",
    "    graph_seed=2026,\n",
    "    seed=2026,\n",
    "    decision_coef=0.8,\n",
    "    repayment_coef=0.8,\n",
    " )\n",
    "\n",
    "compute_statistics(s_test, Xs, Ds, fm, OYs=Ys,As=As) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb056d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained parameters to disk (portable: NumPy .npz)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ckpt_dir = Path(\"checkpoints\")\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "params_path = ckpt_dir / \"fairmodel_params.npz\"\n",
    "state_path  = ckpt_dir / \"fairmodel_state_dict.pt\"\n",
    "\n",
    "theta = np.asarray(fm.params, dtype=np.float32)  # shape: (n_features + 1,)\n",
    "\n",
    "np.savez(\n",
    "    params_path,\n",
    "    params=theta,\n",
    "    n_features=int(fm.linear.weight.shape[1]),\n",
    ")\n",
    "\n",
    "# Optional: save full torch state_dict too (lets you restore optimizer, etc. if you want later)\n",
    "torch.save(fm.state_dict(), state_path)\n",
    "\n",
    "print(f\"Saved params to: {params_path.resolve()}\")\n",
    "print(f\"Saved state_dict to: {state_path.resolve()}\")\n",
    "print(\"theta shape:\", theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12140c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters somewhere else and apply to a fresh FairModel\n",
    "# (Works even in a different notebook/script, as long as you have the same n_features)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_params_npz(npz_path):\n",
    "    npz_path = Path(npz_path)\n",
    "    data = np.load(npz_path)\n",
    "    return np.asarray(data[\"params\"], dtype=np.float32)\n",
    "\n",
    "\n",
    "def apply_theta_to_fairmodel(model: FairModel, theta: np.ndarray) -> None:\n",
    "    \"\"\"theta is [w0, w1, ..., w_{d-1}, b] matching FairModel.params.\"\"\"\n",
    "    theta = np.asarray(theta, dtype=np.float32).ravel()\n",
    "    d = int(model.linear.weight.shape[1])\n",
    "    if theta.shape[0] != d + 1:\n",
    "        raise ValueError(f\"theta has len {theta.shape[0]} but expected {d + 1} (d={d})\")\n",
    "\n",
    "    w = torch.from_numpy(theta[:d]).view(1, d)\n",
    "    b = torch.tensor([float(theta[-1])], dtype=w.dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.linear.weight.copy_(w)\n",
    "        model.linear.bias.copy_(b)\n",
    "\n",
    "    # keep old_* in sync with the loaded weights\n",
    "    model.save_params()\n",
    "\n",
    "\n",
    "params_path = Path(\"checkpoints\") / \"fairmodel_params.npz\"\n",
    "loaded_theta = load_params_npz(params_path)\n",
    "\n",
    "# Example: create a new model with the same n_features and load the theta\n",
    "fm_loaded = FairModel(\n",
    "    n_features=int(fm.linear.weight.shape[1]),\n",
    "    lr=5e-3,\n",
    "    l2_reg=1e-5,\n",
    "    sf_reg=0.119,\n",
    "    lf_reg=0.154,\n",
    ")\n",
    "apply_theta_to_fairmodel(fm_loaded, loaded_theta)\n",
    "\n",
    "print(\"Loaded theta OK. First 5 entries:\", loaded_theta[:5])\n",
    "print(\"Params match:\", np.allclose(fm_loaded.params, loaded_theta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
